{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6e8491f",
   "metadata": {},
   "source": [
    "# ========================================== THEORY ==================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fb8a83",
   "metadata": {},
   "source": [
    "# Q1: Explain the working principle of KNN.\n",
    "\"\"\"\n",
    "**K-Nearest Neighbors (KNN)** is a lazy, instance-based algorithm that classifies a data point based on how its neighbors are classified.\n",
    "It calculates the distance between the query point and all other points in the dataset, selects the 'k' nearest points, and assigns the most common class among them.\n",
    "It is best suited for **classification** and **regression problems**, especially when the decision boundary is non-linear.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f66de",
   "metadata": {},
   "source": [
    "# Q2: Common Distance Metrics in KNN.\n",
    "\"\"\"\n",
    "- **Euclidean Distance**: Most common; calculates straight-line distance.  \n",
    "- **Manhattan Distance**: Sum of absolute differences across dimensions.  \n",
    "- **Minkowski Distance**: Generalized version of both Euclidean and Manhattan (with parameter `p`).  \n",
    "- **Hamming Distance**: For categorical variables (e.g., strings or binary features).  \n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a0d722",
   "metadata": {},
   "source": [
    "# Q3: Advantages and Limitations of KNN\n",
    "\"\"\"\n",
    "**Advantages**:\n",
    "- Simple to understand and implement.\n",
    "- No training phase – instant learning.\n",
    "- Works well with small datasets.\n",
    "\n",
    "**Limitations**:\n",
    "- Computationally expensive for large datasets.\n",
    "- Sensitive to irrelevant features and scaling.\n",
    "- Struggles with imbalanced classes.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fabf68",
   "metadata": {},
   "source": [
    "# Q4: Bayes' Theorem\n",
    "\"\"\"\n",
    "Bayes' Theorem:  \n",
    "$$P(A|B) = \\\\frac{P(B|A) * P(A)}{P(B)}$$  \n",
    "In Naive Bayes, it helps calculate the probability of a class given the input features, assuming independence between features.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bddc4e",
   "metadata": {},
   "source": [
    "# Q5: What does “Naive” mean?\n",
    "\"\"\"\n",
    "The term **\"Naive\"** assumes that all features are **independent** of each other given the class label.\n",
    "This simplifies computation and allows scalability, even though it's rarely true in practice.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec05486b",
   "metadata": {},
   "source": [
    "# Q6: Compare Gaussian, Multinomial, Bernoulli\n",
    "\"\"\"\n",
    "- **GaussianNB**: Assumes features follow a normal distribution. Used for continuous data (e.g., Iris).\n",
    "- **MultinomialNB**: Used for count data (e.g., word frequencies in text classification).\n",
    "- **BernoulliNB**: Binary features (e.g., presence/absence of words).\n",
    "\n",
    "**Use Cases**:\n",
    "- Gaussian: Iris dataset (classification by petal/sepal features).\n",
    "- Multinomial: Spam filtering, document classification.\n",
    "- Bernoulli: Sentiment analysis (binary presence of certain words).\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d69005",
   "metadata": {},
   "source": [
    "# Q7: Two Key Differences\n",
    "\"\"\"\n",
    "1. **Learning Type**:  \n",
    "   - KNN is a lazy learner (no training)  \n",
    "   - Naive Bayes is a probabilistic model (trained using statistics)\n",
    "\n",
    "2. **Performance**:  \n",
    "   - KNN struggles with high-dimensional data  \n",
    "   - Naive Bayes handles high-dimensional, sparse data well (e.g., text)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af78fbc1",
   "metadata": {},
   "source": [
    "# ========================================== PRACTICAL PART A ==================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and Split Iris Data\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6818b630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f11df9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "KNN with k=3\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "\n",
      "KNN with k=5\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n",
      "\n",
      "KNN with k=7\n",
      "Accuracy: 1.0\n",
      "Confusion Matrix:\n",
      " [[10  0  0]\n",
      " [ 0  9  0]\n",
      " [ 0  0 11]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# KNN Training & Evaluation (k=3,5,7)\n",
    "for k in [3, 5, 7]:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_scaled, y_train)\n",
    "    y_pred = knn.predict(X_test_scaled)\n",
    "    \n",
    "    print(f\"\\nKNN with k={k}\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5412d5fb",
   "metadata": {},
   "source": [
    "# ========================================== PRACTICAL PART B ==================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ddb71392",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            message\n",
       "0      0  Go until jurong point, crazy.. Available only ...\n",
       "1      0                      Ok lar... Joking wif u oni...\n",
       "2      1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      0  U dun say so early hor... U c already then say...\n",
       "4      0  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load dataset and keep only needed columns\n",
    "df = pd.read_csv(\"spam.csv\", encoding=\"latin-1\")[['Category', 'Message']]\n",
    "df.columns = ['label', 'message']  # Rename for clarity\n",
    "\n",
    "# Convert labels to binary\n",
    "df['label'] = df['label'].map({'ham': 0, 'spam': 1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63893387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess Text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
    "    return text\n",
    "\n",
    "df['cleaned'] = df['message'].apply(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72d0c97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['cleaned'])\n",
    "y = df['label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB Performance:\n",
      "Accuracy: 0.9551569506726457\n",
      "Confusion Matrix:\n",
      " [[966   0]\n",
      " [ 50  99]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      1.00      0.97       966\n",
      "           1       1.00      0.66      0.80       149\n",
      "\n",
      "    accuracy                           0.96      1115\n",
      "   macro avg       0.98      0.83      0.89      1115\n",
      "weighted avg       0.96      0.96      0.95      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB\n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(X_train, y_train)\n",
    "y_pred_mnb = mnb.predict(X_test)\n",
    "\n",
    "print(\"MultinomialNB Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_mnb))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_mnb))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_mnb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9de7954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB Performance:\n",
      "Accuracy: 0.979372197309417\n",
      "Confusion Matrix:\n",
      " [[963   3]\n",
      " [ 20 129]]\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      1.00      0.99       966\n",
      "           1       0.98      0.87      0.92       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.98      0.93      0.95      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# BernoulliNB\n",
    "bnb = BernoulliNB()\n",
    "bnb.fit(X_train, y_train)\n",
    "y_pred_bnb = bnb.predict(X_test)\n",
    "\n",
    "print(\"BernoulliNB Performance:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_bnb))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_bnb))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_bnb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n- **False Positives (Ham → Spam)**: 0 → Legit messages marked spam.  \\n- **False Negatives (Spam → Ham)**: 50 → Spam messages passed as legit.  \\n\\n**Ideal Goal**: Minimize false negatives for better spam filtering.\\n'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interpret Errors\n",
    "cm = confusion_matrix(y_test, y_pred_mnb)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "display(f\"\"\"\n",
    "- **False Positives (Ham → Spam)**: {fp} → Legit messages marked spam.  \n",
    "- **False Negatives (Spam → Ham)**: {fn} → Spam messages passed as legit.  \n",
    "\n",
    "**Ideal Goal**: Minimize false negatives for better spam filtering.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0aadf481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB on Iris:\n",
      "Accuracy: 1.0\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00         9\n",
      "           2       1.00      1.00      1.00        11\n",
      "\n",
      "    accuracy                           1.00        30\n",
      "   macro avg       1.00      1.00      1.00        30\n",
      "weighted avg       1.00      1.00      1.00        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reload and resplit the Iris dataset to avoid contamination\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load Iris again\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Gaussian Naive Bayes\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "y_pred_gnb = gnb.predict(X_test_scaled)\n",
    "\n",
    "# Evaluation\n",
    "print(\"GaussianNB on Iris:\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gnb))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred_gnb))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "91f4ed73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN Cross-validation (k=5): 0.9733333333333334\n",
      "GaussianNB Cross-validation: 0.9533333333333334\n"
     ]
    }
   ],
   "source": [
    "# Cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "print(\"KNN Cross-validation (k=5):\", cross_val_score(KNeighborsClassifier(5), X, y, cv=5).mean())\n",
    "print(\"GaussianNB Cross-validation:\", cross_val_score(GaussianNB(), X, y, cv=5).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c33cf2c",
   "metadata": {},
   "source": [
    "# =========================================== FINAL SUMMARY ===================================\n",
    "\"\"\"\n",
    "## ✅ Final Observations:\n",
    "\n",
    "- **KNN** achieved high accuracy on Iris but is sensitive to feature scaling and not ideal for high-dimensional data.\n",
    "- **Naive Bayes (Multinomial)** worked well for spam classification due to handling of word frequencies.\n",
    "- **BernoulliNB** was slightly less accurate but useful for binary word presence.\n",
    "- **GaussianNB** performed well on Iris, comparable to KNN.\n",
    "\n",
    "### 🔍 Conclusion:\n",
    "Choose **KNN** for structured, numeric data; **Naive Bayes** for text classification or when speed is important.\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
